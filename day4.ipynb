{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 1 # 0 - low, 1 - high\n",
    "\n",
    "class MDPRecycling: # Markov Decision Processes\n",
    "  def __init__(self, state = 1, alfa = 0.4, beta = 0.2):  #alfa, beta -> bajar baterÃ­a, agotar\n",
    "    self.state = state\n",
    "    self.alfa = alfa\n",
    "    self.beta = beta\n",
    "    self.reward = 0\n",
    "    self.step = 0\n",
    "  \n",
    "  def action(self, action): # action can be s, w, r -> search, wait, recharge\n",
    "    \n",
    "    if self.state == 1:\n",
    "      if action == 's':\n",
    "        self.reward += 1\n",
    "        if np.random.random() < self.alfa:\n",
    "          self.state = 0\n",
    "      elif action == 'w':\n",
    "        self.reward -= 0.2\n",
    "      elif action == 'r':\n",
    "        \"In the state 'High', you can't recharge.\"\n",
    "        return\n",
    "  \n",
    "    elif self.state == 0:\n",
    "      if action == 's':\n",
    "        if np.random.random() < self.beta:\n",
    "          self.state = 1\n",
    "          self.reward -= 3\n",
    "        else:\n",
    "          self.reward += 1\n",
    "      elif action == 'w':\n",
    "        self.reward -= 0.2\n",
    "      elif action == 'r':\n",
    "        self.state = 1\n",
    "    self.step +=1\n",
    "\n",
    "    print(f\"In step {self.step} with action {action}:\\n\\t state: {self.state}, reward, {self.reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step 1 with action s:\n",
      "\t state: 0, reward, 1\n",
      "In step 2 with action s:\n",
      "\t state: 0, reward, 2\n",
      "In step 3 with action s:\n",
      "\t state: 1, reward, -1\n",
      "In step 4 with action s:\n",
      "\t state: 1, reward, 0\n",
      "In step 5 with action w:\n",
      "\t state: 1, reward, -0.2\n",
      "In step 6 with action w:\n",
      "\t state: 1, reward, -0.4\n",
      "In step 7 with action s:\n",
      "\t state: 1, reward, 0.6\n",
      "In step 8 with action s:\n",
      "\t state: 0, reward, 1.6\n",
      "In step 9 with action s:\n",
      "\t state: 0, reward, 2.6\n",
      "In step 10 with action s:\n",
      "\t state: 0, reward, 3.6\n",
      "In step 11 with action s:\n",
      "\t state: 0, reward, 4.6\n"
     ]
    }
   ],
   "source": [
    "robot = MDPRecycling()\n",
    "\n",
    "actions = ['s', 's', 's', 'r', 's', 'w', 'w', 's', 's', 's', 's', 's']\n",
    "\n",
    "for action in actions:\n",
    "  robot.action(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
